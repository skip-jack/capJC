{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Convolution2D, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "import gdown\n",
    "from deepface.basemodels import VGGFace\n",
    "from deepface.commons import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TensorFlow version for compatibility\n",
    "tf_version = int(tf.__version__.split(\".\", maxsplit=1)[0])\n",
    "if tf_version == 1:\n",
    "    from keras.models import Model, Sequential\n",
    "    from keras.layers import Convolution2D, Flatten, Activation\n",
    "elif tf_version == 2:\n",
    "    from tensorflow.keras.models import Model, Sequential\n",
    "    from tensorflow.keras.layers import Convolution2D, Flatten, Activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_mapping = {\n",
    "    0: \"White\",\n",
    "    1: \"Black\",\n",
    "    2: \"Asian\",\n",
    "    3: \"Indian\",\n",
    "    4: \"Others\"  # Others include Hispanic, Latino, Middle Eastern, etc.\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess a single image\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (200, 200))\n",
    "    return img_to_array(img) / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse filename and extract labels\n",
    "def parse_labels(filename):\n",
    "    parts = os.path.basename(filename).split(\"_\")\n",
    "    age = int(parts[0])\n",
    "    gender = int(parts[1])\n",
    "    race = int(parts[2])\n",
    "    return age, gender, race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generator\n",
    "\n",
    "def safe_parse_labels(filename):\n",
    "    try:\n",
    "        parts = os.path.basename(filename).split(\"_\")\n",
    "        if len(parts) < 3:  # Check if filename has enough parts\n",
    "            return None\n",
    "        age = int(parts[0])\n",
    "        gender = int(parts[1])\n",
    "        race = int(parts[2])\n",
    "        return age, gender, race\n",
    "    except ValueError:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        batch_paths = np.random.choice(a=file_paths, size=batch_size)\n",
    "        batch_input = []\n",
    "        batch_age_output = []\n",
    "\n",
    "        for input_path in batch_paths:\n",
    "            labels = safe_parse_labels(input_path)\n",
    "            if labels is None:  # Skip files with incorrect format\n",
    "                continue\n",
    "            image = preprocess_image(input_path)\n",
    "            age, _, _ = labels\n",
    "            batch_input.append(image)\n",
    "            batch_age_output.append(age)\n",
    "\n",
    "        if not batch_input:  # Skip batch if empty\n",
    "            continue\n",
    "\n",
    "        yield np.array(batch_input, dtype='float32'), np.array(batch_age_output, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        batch_paths = np.random.choice(a=file_paths, size=batch_size)\n",
    "        batch_input = []\n",
    "        batch_gender_output = []\n",
    "\n",
    "        for input_path in batch_paths:\n",
    "            labels = safe_parse_labels(input_path)\n",
    "            if labels is None:  # Skip files with incorrect format\n",
    "                continue\n",
    "            image = preprocess_image(input_path)\n",
    "            _, gender, _ = labels\n",
    "            batch_input.append(image)\n",
    "            batch_gender_output.append(gender)\n",
    "\n",
    "        if not batch_input:  # Skip batch if empty\n",
    "            continue\n",
    "\n",
    "        yield np.array(batch_input, dtype='float32'), np.array(batch_gender_output, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def race_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        batch_paths = np.random.choice(a=file_paths, size=batch_size)\n",
    "        batch_input = []\n",
    "        batch_race_output = []\n",
    "\n",
    "        for input_path in batch_paths:\n",
    "            labels = safe_parse_labels(input_path)\n",
    "            if labels is None:  # Skip files with incorrect format\n",
    "                continue\n",
    "            image = preprocess_image(input_path)\n",
    "            _, _, race = labels\n",
    "            batch_input.append(image)\n",
    "            batch_race_output.append(race)\n",
    "\n",
    "        if not batch_input:  # Skip batch if empty\n",
    "            continue\n",
    "\n",
    "        # Convert race to categorical format\n",
    "        batch_race_output_categorical = to_categorical(batch_race_output, num_classes=len(race_mapping))\n",
    "\n",
    "        yield np.array(batch_input, dtype='float32'), batch_race_output_categorical\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        X = np.array(batch_input, dtype='float32')\n",
    "        Y_age = np.array(batch_age_output, dtype='float32')\n",
    "        Y_gender = np.array(batch_gender_output, dtype='float32')\n",
    "        Y_ethnicity = to_categorical(batch_ethnicity_output, num_classes=5)\n",
    "\n",
    "        yield X, {'age_output': Y_age, 'gender_output': Y_gender, 'ethnicity_output': Y_ethnicity}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your images directory\n",
    "images_directory = 'E:/capJC/paper/part1'  # Update with your path\n",
    "img_paths = glob(os.path.join(images_directory, \"*.jpg\"))\n",
    "\n",
    "# Splitting dataset into training and testing\n",
    "train_paths, test_paths = train_test_split(img_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32  # Adjust this based on your memory constraints\n",
    "\n",
    "# Training data generator\n",
    "# Training data generators\n",
    "train_age_generator = age_data_generator(train_paths, batch_size)\n",
    "train_gender_generator = gender_data_generator(train_paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the age_model\n",
    "age_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, name='age_output')\n",
    "])\n",
    "age_model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gender_model\n",
    "gender_model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid', name='gender_output')\n",
    "])\n",
    "gender_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-built model for ethnicity prediction\n",
    "def load_ethnicity_model(url=\"https://github.com/serengil/deepface_models/releases/download/v1.0/race_model_single_batch.h5\"):\n",
    "    model = VGGFace.baseModel()\n",
    "    classes = 6\n",
    "    base_model_output = Sequential()\n",
    "    base_model_output = Convolution2D(classes, (1, 1), name=\"predictions\")(model.layers[-4].output)\n",
    "    base_model_output = Flatten()(base_model_output)\n",
    "    base_model_output = Activation(\"softmax\")(base_model_output)\n",
    "    race_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "\n",
    "    # Load weights\n",
    "    home = functions.get_deepface_home()\n",
    "    output = home + \"/.deepface/weights/race_model_single_batch.h5\"\n",
    "    if not os.path.isfile(output):\n",
    "        print(\"race_model_single_batch.h5 will be downloaded...\")\n",
    "        gdown.download(url, output, quiet=False)\n",
    "    race_model.load_weights(output)\n",
    "    return race_model\n",
    "\n",
    "ethnicity_model = load_ethnicity_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        batch_paths = np.random.choice(a=file_paths, size=batch_size)\n",
    "        batch_input = []\n",
    "        batch_age_output = []\n",
    "\n",
    "        for input_path in batch_paths:\n",
    "            image = preprocess_image(input_path)\n",
    "            age, _, _ = parse_labels(input_path)\n",
    "\n",
    "            batch_input.append(image)\n",
    "            batch_age_output.append(age)\n",
    "\n",
    "        X = np.array(batch_input, dtype='float32')\n",
    "        Y_age = np.array(batch_age_output, dtype='float32')\n",
    "\n",
    "        yield X, Y_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        batch_paths = np.random.choice(a=file_paths, size=batch_size)\n",
    "        batch_input = []\n",
    "        batch_gender_output = []\n",
    "\n",
    "        for input_path in batch_paths:\n",
    "            image = preprocess_image(input_path)\n",
    "            _, gender, _ = parse_labels(input_path)\n",
    "\n",
    "            batch_input.append(image)\n",
    "            batch_gender_output.append(gender)\n",
    "\n",
    "        X = np.array(batch_input, dtype='float32')\n",
    "        Y_gender = np.array(batch_gender_output, dtype='float32')\n",
    "\n",
    "        yield X, Y_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generators\n",
    "train_age_generator = age_data_generator(train_paths, batch_size)\n",
    "train_gender_generator = gender_data_generator(train_paths, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      " 12/602 [..............................] - ETA: 7:42 - loss: 2163.9846 - mae: 37.3752"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nValueError: invalid literal for int() with base 10: ''\nTraceback (most recent call last):\n\n  File \"C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\hvbvm\\AppData\\Local\\Temp\\ipykernel_10664\\1484312716.py\", line 9, in age_data_generator\n    age, _, _ = parse_labels(input_path)\n                ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\hvbvm\\AppData\\Local\\Temp\\ipykernel_10664\\761029784.py\", line 5, in parse_labels\n    gender = int(parts[1])\n             ^^^^^^^^^^^^^\n\nValueError: invalid literal for int() with base 10: ''\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_1730]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 17\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m steps_per_epoch \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_paths) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m batch_size\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Train the age model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m history_age \u001b[39m=\u001b[39m age_model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_age_generator, \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch, \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Train the gender model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m history_gender \u001b[39m=\u001b[39m gender_model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     train_gender_generator, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39msteps_per_epoch, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nValueError: invalid literal for int() with base 10: ''\nTraceback (most recent call last):\n\n  File \"C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\hvbvm\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\data_adapter.py\", line 917, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\hvbvm\\AppData\\Local\\Temp\\ipykernel_10664\\1484312716.py\", line 9, in age_data_generator\n    age, _, _ = parse_labels(input_path)\n                ^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\hvbvm\\AppData\\Local\\Temp\\ipykernel_10664\\761029784.py\", line 5, in parse_labels\n    gender = int(parts[1])\n             ^^^^^^^^^^^^^\n\nValueError: invalid literal for int() with base 10: ''\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_1730]"
     ]
    }
   ],
   "source": [
    "# Calculate steps_per_epoch\n",
    "steps_per_epoch = len(train_paths) // batch_size\n",
    "\n",
    "# Train the age model\n",
    "history_age = age_model.fit(\n",
    "    train_age_generator, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Train the gender model\n",
    "history_gender = gender_model.fit(\n",
    "    train_gender_generator, \n",
    "    steps_per_epoch=steps_per_epoch, \n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate the Models and Visualize Performance Metrics\n",
    "# Evaluate Age Model\n",
    "\n",
    "#Age-test generator:\n",
    "\n",
    "def age_test_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(file_paths), batch_size):\n",
    "            batch_paths = file_paths[i:i + batch_size]\n",
    "            batch_input = []\n",
    "            batch_age_output = []\n",
    "\n",
    "            for input_path in batch_paths:\n",
    "                labels = safe_parse_labels(input_path)\n",
    "                if labels is None:  # Skip files with incorrect format\n",
    "                    continue\n",
    "                image = preprocess_image(input_path)\n",
    "                age, _, _ = labels\n",
    "                batch_input.append(image)\n",
    "                batch_age_output.append(age)\n",
    "\n",
    "            yield np.array(batch_input, dtype='float32'), np.array(batch_age_output, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 39s 258ms/step - loss: 378.1849 - mae: 14.8367\n",
      "Age Model - Loss: 378.18487548828125 MAE: 14.836714744567871\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the age_model:\n",
    "\n",
    "# Create test data generator for age\n",
    "test_age_generator = age_test_data_generator(test_paths, batch_size)\n",
    "\n",
    "# Calculate the number of steps needed for the test generator\n",
    "num_test_steps = len(test_paths) // batch_size + (len(test_paths) % batch_size != 0)\n",
    "\n",
    "# Evaluate Age Model\n",
    "age_loss, age_mae = age_model.evaluate(test_age_generator, steps=num_test_steps)\n",
    "print(\"Age Model - Loss:\", age_loss, \"MAE:\", age_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Gender_Model:\n",
    "\n",
    "\n",
    "def gender_test_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(file_paths), batch_size):\n",
    "            batch_paths = file_paths[i:i + batch_size]\n",
    "            batch_input = []\n",
    "            batch_gender_output = []\n",
    "\n",
    "            for input_path in batch_paths:\n",
    "                labels = safe_parse_labels(input_path)\n",
    "                if labels is None:  # Skip files with incorrect format\n",
    "                    continue\n",
    "                image = preprocess_image(input_path)\n",
    "                _, gender, _ = labels\n",
    "                batch_input.append(image)\n",
    "                batch_gender_output.append(gender)\n",
    "\n",
    "            yield np.array(batch_input, dtype='float32'), np.array(batch_gender_output, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151/151 [==============================] - 33s 216ms/step - loss: 1.4925 - accuracy: 0.6804\n",
      "Gender Model - Loss: 1.4925291538238525 Accuracy: 0.6804230809211731\n"
     ]
    }
   ],
   "source": [
    "# Create test data generator for gender\n",
    "test_gender_generator = gender_test_data_generator(test_paths, batch_size)\n",
    "\n",
    "# Calculate the number of steps needed for the test generator\n",
    "num_test_steps = len(test_paths) // batch_size + (len(test_paths) % batch_size != 0)\n",
    "\n",
    "# Evaluate Gender Model\n",
    "gender_loss, gender_acc = gender_model.evaluate(test_gender_generator, steps=num_test_steps)\n",
    "print(\"Gender Model - Loss:\", gender_loss, \"Accuracy:\", gender_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator for ethncity_detect:\n",
    "def ethnicity_test_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(file_paths), batch_size):\n",
    "            batch_paths = file_paths[i:i + batch_size]\n",
    "            batch_input = []\n",
    "\n",
    "            for input_path in batch_paths:\n",
    "                image = preprocess_image(input_path)\n",
    "                batch_input.append(image)\n",
    "\n",
    "            yield np.array(batch_input, dtype='float32')\n",
    "\n",
    "# Create test data generator for ethnicity\n",
    "test_ethnicity_generator = ethnicity_test_data_generator(test_paths, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ethnicity- Modified Image Preprocessing Function\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))  # Resize to (224, 224)\n",
    "    return img_to_array(img) / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ethnicity_test_data_generator(file_paths, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(file_paths), batch_size):\n",
    "            batch_paths = file_paths[i:i + batch_size]\n",
    "            batch_input = []\n",
    "\n",
    "            for input_path in batch_paths:\n",
    "                image = preprocess_image(input_path)\n",
    "                batch_input.append(image)\n",
    "\n",
    "            yield np.array(batch_input, dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m accuracy\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Example usage:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# Assuming 'ethnicity_model' is your trained Keras model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# Assuming 'test_paths' is a list of file paths for the test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Assuming 'batch_size' is defined\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m accuracy \u001b[39m=\u001b[39m calculate_accuracy(test_paths, ethnicity_data_generator, ethnicity_model, batch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEthnicity prediction accuracy: \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     images, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(generator)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     predictions_batch \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     predicted_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(predictions_batch, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X53sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     true_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(labels, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:2655\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2653\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2654\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2655\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2656\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2657\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    875\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 877\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    878\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m    881\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "def ethnicity_data_generator(file_paths, batch_size):\n",
    "    num_files = len(file_paths)\n",
    "    for i in range(0, num_files, batch_size):\n",
    "        batch_paths = file_paths[i:i+batch_size]\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for file_path in batch_paths:\n",
    "            img = preprocess_image(file_path)  # Ensure this is defined and resizes images to (200, 200, 3)\n",
    "            batch_images.append(img)\n",
    "\n",
    "            # Extract labels\n",
    "            parts = file_path.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                race = int(parts[2])\n",
    "                batch_labels.append(race)\n",
    "            else:\n",
    "                # Handle error: skip this file or use a default value\n",
    "                continue\n",
    "\n",
    "        # Yield batch\n",
    "        yield np.array(batch_images), to_categorical(batch_labels, num_classes=len(race_mapping))\n",
    "\n",
    "def calculate_accuracy(file_paths, generator_func, model, batch_size):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    num_steps = len(file_paths) // batch_size\n",
    "\n",
    "    # Generate predictions for full batches\n",
    "    generator = generator_func(file_paths, batch_size)\n",
    "    for _ in range(num_steps):\n",
    "        images, labels = next(generator)\n",
    "        predictions_batch = model.predict(images)\n",
    "        predicted_classes = np.argmax(predictions_batch, axis=1)\n",
    "        true_classes = np.argmax(labels, axis=1)\n",
    "        all_labels.extend(true_classes)\n",
    "        all_predictions.extend(predicted_classes)\n",
    "\n",
    "    # Handle the last partial batch, if there is one\n",
    "    last_batch_size = len(file_paths) % batch_size\n",
    "    if last_batch_size > 0:\n",
    "        # Process the remainder of the files\n",
    "        images, labels = next(generator)\n",
    "        predictions_batch = model.predict(images[:last_batch_size])\n",
    "        predicted_classes = np.argmax(predictions_batch, axis=1)\n",
    "        true_classes = np.argmax(labels[:last_batch_size], axis=1)\n",
    "        all_labels.extend(true_classes)\n",
    "        all_predictions.extend(predicted_classes)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    return accuracy\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'ethnicity_model' is your trained Keras model\n",
    "# Assuming 'test_paths' is a list of file paths for the test set\n",
    "# Assuming 'batch_size' is defined\n",
    "accuracy = calculate_accuracy(test_paths, ethnicity_data_generator, ethnicity_model, batch_size)\n",
    "print(f\"Ethnicity prediction accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m test_ethnicity_generator \u001b[39m=\u001b[39m ethnicity_test_data_generator(test_paths, batch_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Predict ethnicity\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m ethnicity_pred \u001b[39m=\u001b[39m predict_ethnicity(test_ethnicity_generator, num_test_steps)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Obtain actual ethnicity labels for the test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m ethnicity_test_labels \u001b[39m=\u001b[39m [safe_parse_labels(path)[\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m path \u001b[39min\u001b[39;00m test_paths \u001b[39mif\u001b[39;00m safe_parse_labels(path) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 26\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     X_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(test_generator)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     predictions_batch \u001b[39m=\u001b[39m ethnicity_model\u001b[39m.\u001b[39;49mpredict(X_batch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     predictions\u001b[39m.\u001b[39mextend(np\u001b[39m.\u001b[39margmax(predictions_batch, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(predictions)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:2655\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2653\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2654\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 2655\u001b[0m     tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_function(iterator)\n\u001b[0;32m   2656\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   2657\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:877\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    875\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 877\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    878\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m    881\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    882\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1321\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1322\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1323\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[0;32m   1324\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1325\u001b[0m     args,\n\u001b[0;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1327\u001b[0m     executing_eagerly)\n\u001b[0;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1484\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1485\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1487\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1488\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1489\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1490\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1491\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1492\u001b[0m   )\n\u001b[0;32m   1493\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1494\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1495\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1496\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1500\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1501\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ethnicity Prediction and Evaluation\n",
    "\n",
    "def evaluate_model(test_generator_func, model, num_steps, batch_size):\n",
    "    test_generator = test_generator_func(num_test_steps, batch_size)\n",
    "    predictions = []\n",
    "    for _ in range(num_steps):\n",
    "        X_batch, _ = next(test_generator)\n",
    "        predictions_batch = model.predict(X_batch)\n",
    "        predictions.extend(predictions_batch)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Calculate the number of steps needed for the test generator\n",
    "num_test_steps = len(test_paths) // batch_size + (len(test_paths) % batch_size != 0)\n",
    "\n",
    "# Create the test data generator for ethnicity\n",
    "test_ethnicity_generator = ethnicity_test_data_generator(test_paths, batch_size)\n",
    "\n",
    "# Predict ethnicity\n",
    "ethnicity_pred = predict_ethnicity(test_ethnicity_generator, num_test_steps)\n",
    "\n",
    "# Obtain actual ethnicity labels for the test set\n",
    "ethnicity_test_labels = [safe_parse_labels(path)[2] for path in test_paths if safe_parse_labels(path) is not None]\n",
    "ethnicity_test_labels_categorical = to_categorical([race_mapping[race] for race in ethnicity_test_labels], num_classes=len(race_mapping))\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.argmax(ethnicity_test_labels_categorical, axis=1) == ethnicity_pred\n",
    "ethnicity_accuracy = np.mean(correct_predictions)\n",
    "print(\"Ethnicity Prediction Accuracy: {:.2f}%\".format(ethnicity_accuracy * 100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming gender_model is already defined\n",
    "history_gender = gender_model.fit(\n",
    "    X_train, gender_train,\n",
    "    validation_split=0.1,  # using 10% of data for validation\n",
    "    epochs=10,  # Number of epochs\n",
    "    batch_size=32  # Batch size, adjust based on your dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'age_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Assuming age_model is already defined\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m history_age \u001b[39m=\u001b[39m age_model\u001b[39m.\u001b[39mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     X_train, age_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     validation_split\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,  \u001b[39m# using 10% of data for validation\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,  \u001b[39m# Number of epochs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m  \u001b[39m# Batch size, adjust based on your dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'age_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming age_model is already defined\n",
    "history_age = age_model.fit(\n",
    "    X_train, age_train,\n",
    "    validation_split=0.1,  # using 10% of data for validation\n",
    "    epochs=10,  # Number of epochs\n",
    "    batch_size=32  # Batch size, adjust based on your dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Plotting performance metrics for Age Model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m plot_loss(history_age, \u001b[39m'\u001b[39;49m\u001b[39mAge Prediction Model - Loss\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m plot_accuracy(history_age, \u001b[39m'\u001b[39m\u001b[39mAge Prediction Model - MAE\u001b[39m\u001b[39m'\u001b[39m, is_binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Plotting performance metrics for Gender Model\u001b[39;00m\n",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 30\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mval_loss\u001b[39;49m\u001b[39m'\u001b[39;49m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation Loss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(title)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X32sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mEpochs\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot training and validation loss\n",
    "def plot_loss(history, title):\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot training and validation accuracy\n",
    "def plot_accuracy(history, title, is_binary=False):\n",
    "    plt.figure()\n",
    "    acc_key = 'accuracy' if 'accuracy' in history.history else 'acc'\n",
    "    val_acc_key = 'val_accuracy' if 'val_accuracy' in history.history else 'val_acc'\n",
    "    \n",
    "    plt.plot(history.history[acc_key], label='Training Accuracy')\n",
    "    plt.plot(history.history[val_acc_key], label='Validation Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy' if is_binary else 'MAE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting performance metrics for Age Model\n",
    "plot_loss(history_age, 'Age Prediction Model - Loss')\n",
    "plot_accuracy(history_age, 'Age Prediction Model - MAE', is_binary=False)\n",
    "\n",
    "# Plotting performance metrics for Gender Model\n",
    "plot_loss(history_gender, 'Gender Prediction Model - Loss')\n",
    "plot_accuracy(history_gender, 'Gender Prediction Model - Accuracy', is_binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\capJC\\paper\\a.ipynb Cell 32\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m cm \u001b[39m=\u001b[39m confusion_matrix(np\u001b[39m.\u001b[39;49margmax(ethnicity_test_data_generator, axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m), ethnicity_pred)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sns\u001b[39m.\u001b[39mheatmap(cm, annot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fmt\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mg\u001b[39m\u001b[39m'\u001b[39m, xticklabels\u001b[39m=\u001b[39mrace_mapping, yticklabels\u001b[39m=\u001b[39mrace_mapping)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/capJC/paper/a.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mPredicted\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:56\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\core\\fromnumeric.py:45\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[0;32m     47\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# Ethnicity Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(np.argmax(ethnicity_test_data_generator, axis=1), ethnicity_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='g', xticklabels=race_mapping, yticklabels=race_mapping)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Ethnicity Prediction Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
